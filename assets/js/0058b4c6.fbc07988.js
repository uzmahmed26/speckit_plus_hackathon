"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[849],{6164(i){i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/docs/intro","label":"Introduction Of Book","docId":"intro","unlisted":false},{"type":"category","label":"1. Physical AI Fundamentals","items":[{"type":"link","href":"/docs/physical-ai/introduction/","label":"Introduction to Physical AI","docId":"physical-ai/introduction/index","unlisted":false},{"type":"link","href":"/docs/physical-ai/core-principles/","label":"Core Principles of Embodied Intelligence","docId":"physical-ai/core-principles/index","unlisted":false},{"type":"link","href":"/docs/physical-ai/digital-to-physical-transition/","label":"The Digital-to-Physical Transition","docId":"physical-ai/digital-to-physical-transition/index","unlisted":false},{"type":"link","href":"/docs/physical-ai/conclusion/","label":"Conclusion","docId":"physical-ai/conclusion/index","unlisted":false}],"collapsible":true,"collapsed":true,"href":"/docs/category/physical-ai"},{"type":"category","label":"2. ROS 2 (Robot Operating System)","items":[{"type":"link","href":"/docs/ros2/introduction/","label":"Introduction to ROS 2","docId":"ros2/introduction/index","unlisted":false},{"type":"link","href":"/docs/ros2/core-architecture/","label":"Core Architecture","docId":"ros2/core-architecture/index","unlisted":false},{"type":"link","href":"/docs/ros2/communication-patterns/","label":"Communication Patterns","docId":"ros2/communication-patterns/index","unlisted":false},{"type":"link","href":"/docs/ros2/python-integration-rclpy/","label":"Python Integration (rclpy)","docId":"ros2/python-integration-rclpy/index","unlisted":false},{"type":"link","href":"/docs/ros2/urdf/","label":"URDF (Unified Robot Description Format)","docId":"ros2/urdf/index","unlisted":false},{"type":"link","href":"/docs/ros2/conclusion/","label":"Conclusion","docId":"ros2/conclusion/index","unlisted":false}],"collapsible":true,"collapsed":true,"href":"/docs/category/2-ros-2-robot-operating-system"},{"type":"category","label":"3. Robot Simulation","items":[{"type":"link","href":"/docs/simulation/introduction/","label":"The Importance of Simulation in Robotics","docId":"simulation/introduction/index","unlisted":false},{"type":"link","href":"/docs/simulation/platforms/","label":"Simulation Platforms: Gazebo vs. Unity","docId":"simulation/platforms/index","unlisted":false},{"type":"link","href":"/docs/simulation/sensor-simulation/","label":"Sensor Simulation","docId":"simulation/sensor-simulation/index","unlisted":false},{"type":"link","href":"/docs/simulation/conclusion/","label":"Conclusion","docId":"simulation/conclusion/index","unlisted":false}],"collapsible":true,"collapsed":true,"href":"/docs/category/3-robot-simulation"},{"type":"category","label":"5. Vision-Language-Action (VLA)","items":[{"type":"link","href":"/docs/vla/introduction/","label":"The Convergence of Perception, Language, and Action","docId":"vla/introduction/index","unlisted":false},{"type":"link","href":"/docs/vla/pipeline","label":"The VLA Pipeline","docId":"vla/pipeline","unlisted":false},{"type":"link","href":"/docs/vla/llm-role","label":"The Role of Large Language Models in Robotics","docId":"vla/llm-role","unlisted":false},{"type":"link","href":"/docs/vla/conclusion/","label":"Conclusion","docId":"vla/conclusion/index","unlisted":false}],"collapsible":true,"collapsed":true,"href":"/docs/category/5-vision-language-action-vla"},{"type":"category","label":"6. Humanoid Robotics","items":[{"type":"link","href":"/docs/humanoid-robotics/introduction/","label":"The Grand Challenge of Humanoid Robotics","docId":"humanoid-robotics/introduction/index","unlisted":false},{"type":"link","href":"/docs/humanoid-robotics/kinematics-dynamics","label":"Kinematics and Dynamics","docId":"humanoid-robotics/kinematics-dynamics","unlisted":false},{"type":"link","href":"/docs/humanoid-robotics/bipedal-locomotion","label":"Bipedal Locomotion: The Art of Walking","docId":"humanoid-robotics/bipedal-locomotion","unlisted":false},{"type":"link","href":"/docs/humanoid-robotics/manipulation-grasping","label":"Manipulation and Grasping","docId":"humanoid-robotics/manipulation-grasping","unlisted":false},{"type":"link","href":"/docs/humanoid-robotics/human-robot-interaction","label":"Human-Robot Interaction (HRI)","docId":"humanoid-robotics/human-robot-interaction","unlisted":false},{"type":"link","href":"/docs/humanoid-robotics/conclusion/","label":"Conclusion","docId":"humanoid-robotics/conclusion/index","unlisted":false}],"collapsible":true,"collapsed":true,"href":"/docs/category/6-humanoid-robotics"},{"type":"category","label":"7. Hardware Setup","items":[{"type":"link","href":"/docs/hardware/introduction/","label":"Building Your Robotics Lab","docId":"hardware/introduction/index","unlisted":false},{"type":"link","href":"/docs/hardware/workstation-requirements","label":"Workstation Requirements","docId":"hardware/workstation-requirements","unlisted":false},{"type":"link","href":"/docs/hardware/edge-computing-jetson","label":"Edge Computing and the NVIDIA Jetson","docId":"hardware/edge-computing-jetson","unlisted":false},{"type":"link","href":"/docs/hardware/sensors-actuators","label":"Sensors and Actuators","docId":"hardware/sensors-actuators","unlisted":false},{"type":"link","href":"/docs/hardware/lab-infrastructure","label":"Lab Infrastructure","docId":"hardware/lab-infrastructure","unlisted":false},{"type":"link","href":"/docs/hardware/conclusion/","label":"Conclusion","docId":"hardware/conclusion/index","unlisted":false}],"collapsible":true,"collapsed":true,"href":"/docs/category/7-hardware-setup"}]},"docs":{"hardware/conclusion/index":{"id":"hardware/conclusion/index","title":"Conclusion","description":"Setting up the right hardware is a critical first step in any robotics project. By choosing the right workstation, leveraging the power of edge computing, selecting the appropriate sensors and actuators, and outfitting your lab with the necessary infrastructure, you will be well on your way to building the next generation of intelligent robots.","sidebar":"tutorialSidebar"},"hardware/edge-computing-jetson":{"id":"hardware/edge-computing-jetson","title":"Edge Computing and the NVIDIA Jetson","description":"While your workstation is where you will do most of your development, the code you write will ultimately run on a robot. For many modern robotics applications, this means running AI models and other computationally intensive tasks on an \\"edge\\" device\u2014a small, low-power computer that is embedded on the robot itself.","sidebar":"tutorialSidebar"},"hardware/index":{"id":"hardware/index","title":"Hardware Setup","description":"Building Your Robotics Lab"},"hardware/introduction/index":{"id":"hardware/introduction/index","title":"Building Your Robotics Lab","description":"A successful robotics project starts with a solid hardware foundation. Whether you are a student, a researcher, or a hobbyist, having the right hardware and a well-equipped workspace is essential for bringing your robotic creations to life.","sidebar":"tutorialSidebar"},"hardware/lab-infrastructure":{"id":"hardware/lab-infrastructure","title":"Lab Infrastructure","description":"In addition to the robot itself, you will need a well-equipped lab space to support your robotics development.","sidebar":"tutorialSidebar"},"hardware/sensors-actuators":{"id":"hardware/sensors-actuators","title":"Sensors and Actuators","description":"Sensors: The Senses of the Robot","sidebar":"tutorialSidebar"},"hardware/workstation-requirements":{"id":"hardware/workstation-requirements","title":"Workstation Requirements","description":"Your workstation is the command center of your robotics development. It is where you will write code, run simulations, and analyze data. The requirements for a robotics workstation can vary depending on the complexity of your projects, but here are some general guidelines:","sidebar":"tutorialSidebar"},"humanoid-robotics/bipedal-locomotion":{"id":"humanoid-robotics/bipedal-locomotion","title":"Bipedal Locomotion: The Art of Walking","description":"Walking on two legs is a remarkably complex task that humans perform with ease, but it is one of the biggest challenges in humanoid robotics. Bipedal locomotion requires a delicate balance of control, perception, and planning.","sidebar":"tutorialSidebar"},"humanoid-robotics/conclusion/index":{"id":"humanoid-robotics/conclusion/index","title":"Conclusion","description":"Humanoid robotics is a field of immense challenge and opportunity. By bringing together advances in mechanics, control, perception, and AI, we are getting closer to creating robots that can walk, talk, and work alongside us in our daily lives. The journey is long, but the potential rewards are enormous.","sidebar":"tutorialSidebar"},"humanoid-robotics/human-robot-interaction":{"id":"humanoid-robotics/human-robot-interaction","title":"Human-Robot Interaction (HRI)","description":"As humanoid robots become more prevalent in our daily lives, it is crucial that they can interact with humans in a safe, natural, and intuitive way. This is the domain of Human-Robot Interaction (HRI).","sidebar":"tutorialSidebar"},"humanoid-robotics/index":{"id":"humanoid-robotics/index","title":"Humanoid Robotics","description":"The Grand Challenge of Humanoid Robotics"},"humanoid-robotics/introduction/index":{"id":"humanoid-robotics/introduction/index","title":"The Grand Challenge of Humanoid Robotics","description":"Humanoid robots, with their human-like form and capabilities, represent one of the grand challenges of robotics. The goal is to create machines that can operate in human-centric environments, use human tools, and interact with humans in a natural and intuitive way.","sidebar":"tutorialSidebar"},"humanoid-robotics/kinematics-dynamics":{"id":"humanoid-robotics/kinematics-dynamics","title":"Kinematics and Dynamics","description":"The motion of a humanoid robot is governed by the principles of kinematics and dynamics.","sidebar":"tutorialSidebar"},"humanoid-robotics/manipulation-grasping":{"id":"humanoid-robotics/manipulation-grasping","title":"Manipulation and Grasping","description":"For a humanoid robot to be truly useful, it must be able to manipulate objects in its environment. This involves a combination of perception, planning, and control.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction Of Book","description":"Welcome to the ultimate guide to embodied intelligence!","sidebar":"tutorialSidebar"},"physical-ai/conclusion/index":{"id":"physical-ai/conclusion/index","title":"Conclusion","description":"Physical AI is a rapidly advancing field with the potential to revolutionize our world. By creating intelligent agents that can operate in the physical world, we can automate dangerous and tedious tasks, assist the elderly and disabled, and explore new frontiers in science and space.","sidebar":"tutorialSidebar"},"physical-ai/core-principles/index":{"id":"physical-ai/core-principles/index","title":"Core Principles of Embodied Intelligence","description":"The core tenet of Embodied Intelligence is that intelligence is not an abstract property of a disembodied mind, but rather an emergent property of an agent\'s physical interactions with its environment. The body is not just a vessel for the brain; it is an integral part of the cognitive process.","sidebar":"tutorialSidebar"},"physical-ai/digital-to-physical-transition/index":{"id":"physical-ai/digital-to-physical-transition/index","title":"The Digital-to-Physical Transition","description":"Bridging the gap between the digital and physical worlds is one of the biggest challenges in Physical AI. The real world is far more complex and unpredictable than any simulation.","sidebar":"tutorialSidebar"},"physical-ai/index":{"id":"physical-ai/index","title":"Physical AI Fundamentals","description":"Introduction to Physical AI"},"physical-ai/introduction/index":{"id":"physical-ai/introduction/index","title":"Introduction to Physical AI","description":"Physical AI, often used interchangeably with Embodied AI, represents a paradigm shift in the field of artificial intelligence. It moves beyond the confines of digital computation and virtual environments to create intelligent systems that can perceive, reason, and act in the physical world. This is the domain of robots that walk, drones that fly, and autonomous cars that navigate our streets.","sidebar":"tutorialSidebar"},"ros2/communication-patterns/index":{"id":"ros2/communication-patterns/index","title":"Communication Patterns","description":"ROS 2 provides several communication patterns for nodes to exchange data.","sidebar":"tutorialSidebar"},"ros2/conclusion/index":{"id":"ros2/conclusion/index","title":"Conclusion","description":"ROS 2 is a powerful and flexible framework for robotics development. Its modular architecture, robust communication patterns, and strong community support make it an ideal choice for a wide range of robotics applications, from simple hobbyist projects to complex industrial and commercial systems. This chapter has provided a solid foundation for understanding the core concepts of ROS 2. In the following chapters, we will dive deeper into each of these topics and learn how to use them to build our own robotic systems.","sidebar":"tutorialSidebar"},"ros2/core-architecture/index":{"id":"ros2/core-architecture/index","title":"Core Architecture","description":"At its heart, ROS 2 is a distributed system of processes (called \\"nodes\\") that communicate with each other to perform complex tasks.","sidebar":"tutorialSidebar"},"ros2/index":{"id":"ros2/index","title":"ROS 2 (Robot Operating System)","description":"Introduction to ROS 2"},"ros2/introduction/index":{"id":"ros2/introduction/index","title":"Introduction to ROS 2","description":"The Robot Operating System (ROS) is a flexible framework for writing robot software. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.","sidebar":"tutorialSidebar"},"ros2/python-integration-rclpy/index":{"id":"ros2/python-integration-rclpy/index","title":"Python Integration (rclpy)","description":"rclpy is the official Python client library for ROS 2. It provides a Pythonic interface to all the core ROS 2 concepts, allowing developers to write ROS 2 nodes, publishers, subscribers, services, and actions in Python.","sidebar":"tutorialSidebar"},"ros2/urdf/index":{"id":"ros2/urdf/index","title":"URDF (Unified Robot Description Format)","description":"The Unified Robot Description Format (URDF) is an XML format for describing the physical structure of a robot. It is a key component of the ROS ecosystem, used for modeling, simulation, and visualization.","sidebar":"tutorialSidebar"},"simulation/conclusion/index":{"id":"simulation/conclusion/index","title":"Conclusion","description":"Simulation is a critical tool for modern robotics development. Platforms like Gazebo and Unity provide powerful and flexible environments for testing, training, and debugging robots. By understanding the strengths and weaknesses of different platforms and the principles of sensor simulation, you will be well-equipped to leverage simulation to accelerate your robotics projects.","sidebar":"tutorialSidebar"},"simulation/index":{"id":"simulation/index","title":"Robot Simulation","description":"The Importance of Simulation in Robotics"},"simulation/introduction/index":{"id":"simulation/introduction/index","title":"The Importance of Simulation in Robotics","description":"Simulation is an indispensable tool in modern robotics development. It provides a virtual environment where robots can be tested, trained, and debugged without the risk of damaging expensive hardware or endangering humans. The ability to simulate a robot\'s behavior in a controlled and repeatable manner accelerates the development process and enables the exploration of complex scenarios that would be difficult or impossible to test in the real world.","sidebar":"tutorialSidebar"},"simulation/platforms/index":{"id":"simulation/platforms/index","title":"Simulation Platforms: Gazebo vs. Unity","description":"There are several robot simulation platforms available, each with its own strengths and weaknesses. Two of the most popular platforms in the robotics community are Gazebo and Unity.","sidebar":"tutorialSidebar"},"simulation/sensor-simulation/index":{"id":"simulation/sensor-simulation/index","title":"Sensor Simulation","description":"Accurate sensor simulation is crucial for developing and testing perception algorithms. Both Gazebo and Unity provide a wide range of sensor models.","sidebar":"tutorialSidebar"},"vla/conclusion/index":{"id":"vla/conclusion/index","title":"Conclusion","description":"Vision-Language-Action models are a rapidly evolving area of AI with the potential to revolutionize the way we interact with robots. By combining the power of perception, language, and action, we can create truly intelligent agents that can understand our world and our intentions. As these models continue to improve, we can expect to see a new generation of robots that are more capable, more versatile, and more collaborative than ever before.","sidebar":"tutorialSidebar"},"vla/index":{"id":"vla/index","title":"Vision-Language-Action (VLA)","description":"The Convergence of Perception, Language, and Action"},"vla/introduction/index":{"id":"vla/introduction/index","title":"The Convergence of Perception, Language, and Action","description":"Vision-Language-Action (VLA) models represent the cutting edge of AI, bringing together three distinct fields to create truly intelligent agents that can understand and interact with the world in a human-like way. VLAs are the key to building robots that can follow natural language commands, learn new tasks from observation, and collaborate with humans in a shared environment.","sidebar":"tutorialSidebar"},"vla/llm-role":{"id":"vla/llm-role","title":"The Role of Large Language Models in Robotics","description":"The recent advances in Large Language Models have had a transformative impact on the field of robotics. LLMs are not just powerful language models; they are also powerful reasoning engines that can be used to solve a wide range of robotics problems.","sidebar":"tutorialSidebar"},"vla/pipeline":{"id":"vla/pipeline","title":"The VLA Pipeline","description":"A typical VLA pipeline consists of three main stages:","sidebar":"tutorialSidebar"}}}}')}}]);