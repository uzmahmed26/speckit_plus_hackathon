"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[3293],{8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var a=t(6540);const o={},i=a.createContext(o);function r(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(i.Provider,{value:n},e.children)}},9189:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"vla/introduction/index","title":"The Convergence of Perception, Language, and Action","description":"Vision-Language-Action (VLA) models represent the cutting edge of AI, bringing together three distinct fields to create truly intelligent agents that can understand and interact with the world in a human-like way. VLAs are the key to building robots that can follow natural language commands, learn new tasks from observation, and collaborate with humans in a shared environment.","source":"@site/docs/vla/introduction/index.md","sourceDirName":"vla/introduction","slug":"/vla/introduction/","permalink":"/speckit_plus_hackathon/docs/vla/introduction/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/introduction/index.md","tags":[],"version":"current","frontMatter":{"title":"The Convergence of Perception, Language, and Action"},"sidebar":"tutorialSidebar","previous":{"title":"5. Vision-Language-Action (VLA)","permalink":"/speckit_plus_hackathon/docs/category/5-vision-language-action-vla"},"next":{"title":"The VLA Pipeline","permalink":"/speckit_plus_hackathon/docs/vla/pipeline"}}');var o=t(4848),i=t(8453);const r={title:"The Convergence of Perception, Language, and Action"},s="The Convergence of Perception, Language, and Action",c={},l=[];function d(e){const n={h1:"h1",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"the-convergence-of-perception-language-and-action",children:"The Convergence of Perception, Language, and Action"})}),"\n",(0,o.jsx)(n.p,{children:"Vision-Language-Action (VLA) models represent the cutting edge of AI, bringing together three distinct fields to create truly intelligent agents that can understand and interact with the world in a human-like way. VLAs are the key to building robots that can follow natural language commands, learn new tasks from observation, and collaborate with humans in a shared environment."}),"\n",(0,o.jsx)(n.p,{children:"The VLA paradigm is based on a simple but powerful idea: to act intelligently in the world, an agent must be able to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"See (Vision)"}),": Perceive the environment through visual sensors."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Understand (Language)"}),": Interpret natural language instructions and commands."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Act (Action)"}),": Translate its understanding into physical actions in the world."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This chapter will explore the core concepts of VLAs, the technologies that power them, and the exciting applications they enable."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);