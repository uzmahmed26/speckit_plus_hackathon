"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[3921],{7924:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"vla/index","title":"Vision-Language-Action (VLA)","description":"The Convergence of Perception, Language, and Action","source":"@site/docs/vla/index.md","sourceDirName":"vla","slug":"/vla/","permalink":"/speckit_plus_hackathon/docs/vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/index.md","tags":[],"version":"current","frontMatter":{"title":"Vision-Language-Action (VLA)"}}');var o=t(4848),a=t(8453);const s={title:"Vision-Language-Action (VLA)"},r="Vision-Language-Action (VLA)",l={},c=[{value:"The Convergence of Perception, Language, and Action",id:"the-convergence-of-perception-language-and-action",level:2},{value:"The VLA Pipeline",id:"the-vla-pipeline",level:2},{value:"1. Vision (Perception)",id:"1-vision-perception",level:3},{value:"2. Language (Understanding)",id:"2-language-understanding",level:3},{value:"3. Action (Execution)",id:"3-action-execution",level:3},{value:"The Role of Large Language Models in Robotics",id:"the-role-of-large-language-models-in-robotics",level:2},{value:"LLMs as High-Level Planners",id:"llms-as-high-level-planners",level:3},{value:"LLMs for Dexterous Manipulation",id:"llms-for-dexterous-manipulation",level:3},{value:"LLMs for Human-Robot Interaction",id:"llms-for-human-robot-interaction",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"})}),"\n",(0,o.jsx)(n.h2,{id:"the-convergence-of-perception-language-and-action",children:"The Convergence of Perception, Language, and Action"}),"\n",(0,o.jsx)(n.p,{children:"Vision-Language-Action (VLA) models represent the cutting edge of AI, bringing together three distinct fields to create truly intelligent agents that can understand and interact with the world in a human-like way. VLAs are the key to building robots that can follow natural language commands, learn new tasks from observation, and collaborate with humans in a shared environment."}),"\n",(0,o.jsx)(n.p,{children:"The VLA paradigm is based on a simple but powerful idea: to act intelligently in the world, an agent must be able to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"See (Vision)"}),": Perceive the environment through visual sensors."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Understand (Language)"}),": Interpret natural language instructions and commands."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Act (Action)"}),": Translate its understanding into physical actions in the world."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This chapter will explore the core concepts of VLAs, the technologies that power them, and the exciting applications they enable."}),"\n",(0,o.jsx)(n.h2,{id:"the-vla-pipeline",children:"The VLA Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"A typical VLA pipeline consists of three main stages:"}),"\n",(0,o.jsx)(n.h3,{id:"1-vision-perception",children:"1. Vision (Perception)"}),"\n",(0,o.jsx)(n.p,{children:"The first step in any VLA system is to perceive the environment. This is typically done using cameras, but can also involve other sensors like LiDAR and depth cameras. The goal of the vision module is to extract meaningful information from the raw sensor data, such as:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Detection"}),": Identifying and localizing objects in the scene."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scene Segmentation"}),": Dividing the image into different regions corresponding to different objects or surfaces."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"3D Reconstruction"}),": Building a 3D model of the environment."]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"2-language-understanding",children:"2. Language (Understanding)"}),"\n",(0,o.jsx)(n.p,{children:"Once the robot has a representation of its environment, it needs to understand the user's intent. This is where natural language processing (NLP) comes in."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech-to-Text"}),": The first step is often to convert spoken language into text using a speech-to-text model like ",(0,o.jsx)(n.strong,{children:"OpenAI's Whisper"}),". Whisper is a powerful, open-source model that can transcribe audio with high accuracy, even in noisy environments."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Large Language Models (LLMs)"}),': The transcribed text is then fed into a Large Language Model (LLM), such as GPT-4 or Llama. The LLM acts as a "reasoning engine," interpreting the user\'s command and generating a high-level plan to achieve the desired goal.']}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"3-action-execution",children:"3. Action (Execution)"}),"\n",(0,o.jsx)(n.p,{children:"The final step is to translate the LLM's plan into a sequence of low-level robot actions. This is where the \"rubber meets the road,\" and the robot's physical capabilities come into play."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Tool Use / Function Calling"}),': A key technique for bridging the gap between high-level language and low-level actions is "tool use" or "function calling." The LLM is given a set of "tools" that correspond to the robot\'s capabilities (e.g., ',(0,o.jsx)(n.code,{children:"move_to(x, y)"}),", ",(0,o.jsx)(n.code,{children:"pick_up(object)"}),", ",(0,o.jsx)(n.code,{children:"open_drawer()"}),"). The LLM can then generate a sequence of tool calls to execute the desired task."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Motion Planning"}),": For each action, the robot's motion planner needs to generate a collision-free trajectory for the robot to follow."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Control"}),": The robot's controllers then execute the trajectory by sending commands to the motors."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"the-role-of-large-language-models-in-robotics",children:"The Role of Large Language Models in Robotics"}),"\n",(0,o.jsx)(n.p,{children:"The recent advances in Large Language Models have had a transformative impact on the field of robotics. LLMs are not just powerful language models; they are also powerful reasoning engines that can be used to solve a wide range of robotics problems."}),"\n",(0,o.jsx)(n.h3,{id:"llms-as-high-level-planners",children:"LLMs as High-Level Planners"}),"\n",(0,o.jsx)(n.p,{children:'One of the most exciting applications of LLMs in robotics is as high-level planners. Given a complex, ambiguous command like "clean up the kitchen," an LLM can break it down into a sequence of concrete steps:'}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Find all the dirty dishes."}),"\n",(0,o.jsx)(n.li,{children:"Pick up each dish and place it in the dishwasher."}),"\n",(0,o.jsx)(n.li,{children:"Find the sponge and wipe down the counters."}),"\n",(0,o.jsx)(n.li,{children:"Put the sponge back in the sink."}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"llms-for-dexterous-manipulation",children:"LLMs for Dexterous Manipulation"}),"\n",(0,o.jsx)(n.p,{children:"LLMs can also be used to generate policies for dexterous manipulation tasks, such as grasping and assembling objects. By providing the LLM with a description of the object and the desired goal, it can generate a sequence of motor commands to achieve the task."}),"\n",(0,o.jsx)(n.h3,{id:"llms-for-human-robot-interaction",children:"LLMs for Human-Robot Interaction"}),"\n",(0,o.jsx)(n.p,{children:"LLMs are also a powerful tool for improving human-robot interaction. By using an LLM to power a robot's conversational abilities, we can create robots that can understand and respond to natural language in a more human-like way."}),"\n",(0,o.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(n.p,{children:"Vision-Language-Action models are a rapidly evolving area of AI with the potential to revolutionize the way we interact with robots. By combining the power of perception, language, and action, we can create truly intelligent agents that can understand our world and our intentions. As these models continue to improve, we can expect to see a new generation of robots that are more capable, more versatile, and more collaborative than ever before."})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);