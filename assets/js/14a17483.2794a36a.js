"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[9900],{1650:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"vla/pipeline","title":"The VLA Pipeline","description":"A typical VLA pipeline consists of three main stages:","source":"@site/docs/vla/pipeline.md","sourceDirName":"vla","slug":"/vla/pipeline","permalink":"/speckit_plus_hackathon/docs/vla/pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/pipeline.md","tags":[],"version":"current","frontMatter":{"title":"The VLA Pipeline"},"sidebar":"tutorialSidebar","previous":{"title":"The Convergence of Perception, Language, and Action","permalink":"/speckit_plus_hackathon/docs/vla/introduction/"},"next":{"title":"The Role of Large Language Models in Robotics","permalink":"/speckit_plus_hackathon/docs/vla/llm-role"}}');var o=t(4848),s=t(8453);const r={title:"The VLA Pipeline"},a="The VLA Pipeline",l={},c=[{value:"1. Vision (Perception)",id:"1-vision-perception",level:2},{value:"2. Language (Understanding)",id:"2-language-understanding",level:2},{value:"3. Action (Execution)",id:"3-action-execution",level:2}];function h(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"the-vla-pipeline",children:"The VLA Pipeline"})}),"\n",(0,o.jsx)(n.p,{children:"A typical VLA pipeline consists of three main stages:"}),"\n",(0,o.jsx)(n.h2,{id:"1-vision-perception",children:"1. Vision (Perception)"}),"\n",(0,o.jsx)(n.p,{children:"The first step in any VLA system is to perceive the environment. This is typically done using cameras, but can also involve other sensors like LiDAR and depth cameras. The goal of the vision module is to extract meaningful information from the raw sensor data, such as:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Detection"}),": Identifying and localizing objects in the scene."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scene Segmentation"}),": Dividing the image into different regions corresponding to different objects or surfaces."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"3D Reconstruction"}),": Building a 3D model of the environment."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"2-language-understanding",children:"2. Language (Understanding)"}),"\n",(0,o.jsx)(n.p,{children:"Once the robot has a representation of its environment, it needs to understand the user's intent. This is where natural language processing (NLP) comes in."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech-to-Text"}),": The first step is often to convert spoken language into text using a speech-to-text model like ",(0,o.jsx)(n.strong,{children:"OpenAI's Whisper"}),". Whisper is a powerful, open-source model that can transcribe audio with high accuracy, even in noisy environments."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Large Language Models (LLMs)"}),': The transcribed text is then fed into a Large Language Model (LLM), such as GPT-4 or Llama. The LLM acts as a "reasoning engine," interpreting the user\'s command and generating a high-level plan to achieve the desired goal.']}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"3-action-execution",children:"3. Action (Execution)"}),"\n",(0,o.jsx)(n.p,{children:"The final step is to translate the LLM's plan into a sequence of low-level robot actions. This is where the \"rubber meets the road,\" and the robot's physical capabilities come into play."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Tool Use / Function Calling"}),': A key technique for bridging the gap between high-level language and low-level actions is "tool use" or "function calling." The LLM is given a set of "tools" that correspond to the robot\'s capabilities (e.g., ',(0,o.jsx)(n.code,{children:"move_to(x, y)"}),", ",(0,o.jsx)(n.code,{children:"pick_up(object)"}),", ",(0,o.jsx)(n.code,{children:"open_drawer()"}),"). The LLM can then generate a sequence of tool calls to execute the desired task."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Motion Planning"}),": For each action, the robot's motion planner needs to generate a collision-free trajectory for the robot to follow."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Control"}),": The robot's controllers then execute the trajectory by sending commands to the motors."]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);